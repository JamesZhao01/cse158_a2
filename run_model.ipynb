{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm,random, numpy as np, pandas as pd, os, sklearn, pytorch_lightning as pl, torch, torch.nn as nn\r\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from collections import defaultdict\r\n",
    "from IPython.display import display, HTML\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data import Dataset\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosNegDataset(Dataset):\r\n",
    "    # pos_list: positives in dataset\r\n",
    "    # asins: all asins in dataset\r\n",
    "    # all_pos: all positives in the entire dataset\r\n",
    "    # features: all features as df\r\n",
    "    # asin_to_idx: map asin to an idx in features\r\n",
    "    def __init__(self, pos_list, all_pos, all_asins, asin_to_idx, path):\r\n",
    "        self.pos_list = pos_list\r\n",
    "        self.all_pos = all_pos\r\n",
    "        self.all_asins = all_asins\r\n",
    "        self.asin_to_idx = asin_to_idx\r\n",
    "        self.path = path\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.pos_list) * 2\r\n",
    "    def get_feature(self, asin):\r\n",
    "        idx = self.asin_to_idx[asin]\r\n",
    "        return torch.load(f\"{self.path}/feats/feats{idx}.pt\")\r\n",
    "    def generate_neg_pair(self):\r\n",
    "        pair = random.sample(self.all_asins, 2)\r\n",
    "        pair.sort()\r\n",
    "        pair = tuple(pair)\r\n",
    "        if pair in self.all_pos:\r\n",
    "            pair = random.sample(self.all_asins, 2)\r\n",
    "            pair.sort()\r\n",
    "            pair = tuple(pair)\r\n",
    "        return pair\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        if idx < len(self.pos_list):\r\n",
    "            a, b = self.pos_list[idx]\r\n",
    "            return self.get_feature(a), self.get_feature(b), 1\r\n",
    "        a, b = self.generate_neg_pair()\r\n",
    "        return self.get_feature(a), self.get_feature(b), 0\r\n",
    "class Mahalanobis(pl.LightningModule):\r\n",
    "    def __init__(self, X_tr, X_va, positives, all_items, asin_to_idx, path, embedding_dims = 4096, K = 10, c = 2):\r\n",
    "        super().__init__()\r\n",
    "        self.mahal = nn.Linear(embedding_dims, K)\r\n",
    "        self.l = nn.BCEWithLogitsLoss()\r\n",
    "        self.c = c\r\n",
    "        self.X_tr, self.X_va, self.positives, self.all_items, self.asin_to_idx = X_tr, X_va, positives, all_items, asin_to_idx\r\n",
    "        self.path = path\r\n",
    "    def forward(self, user_input, item_input):\r\n",
    "        # bs x k\r\n",
    "        a = self.mahal(user_input - item_input)\r\n",
    "        b = torch.linalg.norm(a, dim=1)\r\n",
    "        out = b - self.c\r\n",
    "        return out\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        i1, i2, l = batch\r\n",
    "        pred = self(i1, i2)\r\n",
    "        loss = self.l(pred, l.float())\r\n",
    "        self.log(\"tr/loss_step\", loss)\r\n",
    "        return loss\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        i1, i2, l = batch\r\n",
    "        pred = self(i1, i2)\r\n",
    "        loss = self.l(pred, l.float()).item()\r\n",
    "        accuracy = sklearn.metrics.accuracy_score(l.cpu(), pred.cpu().flatten() > 0.0)\r\n",
    "        self.log(\"val/loss_step\", loss)\r\n",
    "        self.log(\"val/acc_step\", accuracy)\r\n",
    "        return {\"val_acc\": accuracy, \"val_loss\": loss, \"pred\": pred.cpu().flatten().numpy(), \"label\": l.cpu().flatten().numpy()}\r\n",
    "\r\n",
    "    def training_epoch_end(self, training_step_outputs):\r\n",
    "        avg_loss = np.mean(np.mean(training_step_outputs))\r\n",
    "        self.log(\"tr/loss_epoch\", avg_loss)\r\n",
    "\r\n",
    "    def validation_epoch_end(self, validation_step_outputs):\r\n",
    "        acc, loss, pred, label = zip(*[(t[\"val_acc\"], t[\"val_loss\"], t[\"pred\"], t[\"label\"]) for t in validation_step_outputs])\r\n",
    "        mean_acc, mean_loss = np.mean(acc), np.mean(loss)\r\n",
    "        pred, label = np.array(pred).flatten(), np.array(label).flatten()\r\n",
    "        self.log(\"val/loss_epoch\", mean_loss)\r\n",
    "        self.log(\"val/acc_epoch\", sklearn.metrics.accuracy_score(label, pred > 0.0))\r\n",
    "        self.log(\"val/f1_epoch\", sklearn.metrics.f1_score(label, pred > 0.0))\r\n",
    "\r\n",
    "    def configure_optimizers(self):\r\n",
    "        return torch.optim.Adam(self.parameters())\r\n",
    "\r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(PosNegDataset(self.X_tr, self.positives, self.all_items, self.asin_to_idx, self.path),\r\n",
    "                        batch_size=128, num_workers=0, shuffle=True)\r\n",
    "\r\n",
    "    def val_dataloader(self):\r\n",
    "        return DataLoader(PosNegDataset(self.X_va, self.positives, self.all_items, self.asin_to_idx, self.path),\r\n",
    "                        batch_size=128, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(base_path):\r\n",
    "    os.makedirs(base_path, exist_ok=True)\r\n",
    "    meta, rev = pd.read_json(os.path.join(base_path, \"meta.json\")), pd.read_json(os.path.join(base_path, \"rev.json\"))\r\n",
    "    return meta, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165947it [00:06, 26731.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146593/146593 [00:00<00:00, 869994.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta, rev = load(\"./data/shoes\")\r\n",
    "meta = meta.reset_index()\r\n",
    "meta[\"index\"] = meta.index\r\n",
    "rev = rev.reset_index()\r\n",
    "rev[\"index\"] = rev.index\r\n",
    "i, a = meta[\"index\"], meta[\"asin\"]\r\n",
    "asin_to_idx = dict(zip(a, i))\r\n",
    "\r\n",
    "# 121084 - name, 146593 - user\r\n",
    "all_items = set(meta[\"asin\"])\r\n",
    "positives = set()\r\n",
    "\r\n",
    "\r\n",
    "# for idx, b in tqdm.tqdm(meta.iterrows()):\r\n",
    "#     a1= b[\"asin\"]\r\n",
    "#     related = b[\"related\"]\r\n",
    "#     if related is not None and 'also_bought' in related:\r\n",
    "#         for a2 in related['also_bought']:\r\n",
    "#             if a2 in all_items:\r\n",
    "#                 positives.add((a1,a2,1))\r\n",
    "#     if related is not None and 'bought_together' in related:\r\n",
    "#         for a2 in related['bought_together']:\r\n",
    "#             if a2 in all_items:\r\n",
    "#                 positives.add((a1,a2,1))\r\n",
    "\r\n",
    "rPU = defaultdict(list)\r\n",
    "for idx, b in tqdm.tqdm(rev.iterrows()):\r\n",
    "    asin, user = b[\"asin\"], b[\"reviewerID\"]\r\n",
    "    if asin in all_items:\r\n",
    "        rPU[user].append(asin)\r\n",
    "for ratList in rPU.values():\r\n",
    "    ratList.sort()\r\n",
    "print(len(rPU))\r\n",
    "for user, rats in tqdm.tqdm(rPU.items()):\r\n",
    "    for i in range(len(rats)):\r\n",
    "        for j in range(i + 1, len(rats)):\r\n",
    "            positives.add((rats[i], rats[j]))\r\n",
    "\r\n",
    "positives_li = list(positives)\r\n",
    "print(len(positives_li))\r\n",
    "ones = [1] * len(positives_li)\r\n",
    "\r\n",
    "del meta\r\n",
    "del rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(positives_li, ones, test_size=0.1, random_state=1)\r\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.111, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | mahal | Linear            | 41.0 K\n",
      "1 | l     | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "41.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.0 K    Total params\n",
      "0.164     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num positives 28311 Maximum possible (using combinatorics) 30241.0\n",
      "X_tr: 22650, X_va: 2829, X_te: 2832\n",
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  63%|██████▎   | 250/399 [02:41<01:36,  1.55it/s, loss=0.711, v_num=15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Num positives\", len(positives), \"Maximum possible (using combinatorics)\", sum([len(t) * (len(t) - 1) / 2 for t in rPU.values() if len(t) > 1]))\r\n",
    "print(f\"X_tr: {len(X_tr)}, X_va: {len(X_va)}, X_te: {len(X_te)}\")\r\n",
    "model = Mahalanobis(X_tr, X_va, positives, all_items, asin_to_idx, path = \"./data/shoes\")\r\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/acc_step\")\r\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1, reload_dataloaders_every_epoch=True, progress_bar_refresh_rate=50, logger=True, default_root_dir=\"./models\", callbacks=[checkpoint_callback])\r\n",
    "trainer.fit(model)\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cse158_37': conda)",
   "name": "python3711jvsc74a57bd00d151ab34c9aaab944e38038fcabbcbde94c7261585d9a9c92994237575effe2"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}