{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip, tqdm, math, csv, random, numpy as np, pandas as pd, os\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import matplotlib.lines as lines\r\n",
    "from collections import defaultdict\r\n",
    "from sklearn import linear_model\r\n",
    "from nltk.stem.porter import *\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from string import punctuation\r\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(base_path):\r\n",
    "    os.makedirs(base_path, exist_ok=True)\r\n",
    "    meta, rev = pd.read_json(os.path.join(base_path, \"meta.json\")), pd.read_json(os.path.join(base_path, \"rev.json\"))\r\n",
    "    return meta, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, rev = load(\"./data/shoes\")\r\n",
    "meta = meta.reset_index()\r\n",
    "meta[\"index\"] = meta.index\r\n",
    "rev = rev.reset_index()\r\n",
    "rev[\"index\"] = rev.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57985 165947\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>asin</th>\n      <th>related</th>\n      <th>title</th>\n      <th>price</th>\n      <th>salesRank</th>\n      <th>imUrl</th>\n      <th>brand</th>\n      <th>categories</th>\n      <th>description</th>\n      <th>feats</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>8921463216</td>\n      <td>{'also_viewed': ['8921463267', '8921463208', '...</td>\n      <td>Hello Kitty LALA Lovely Womens Summer Slippers...</td>\n      <td>NaN</td>\n      <td>{'Beauty': 377777}</td>\n      <td>http://ecx.images-amazon.com/images/I/41hbUxgB...</td>\n      <td>None</td>\n      <td>[Clothing, Shoes &amp; Jewelry, Women, Shoes, Slip...</td>\n      <td>None</td>\n      <td>[0.0, 3.3559999466, 0.0, 0.0, 1.5535999537, 0....</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   index        asin                                            related  \\\n0      0  8921463216  {'also_viewed': ['8921463267', '8921463208', '...   \n\n                                               title  price  \\\n0  Hello Kitty LALA Lovely Womens Summer Slippers...    NaN   \n\n            salesRank                                              imUrl  \\\n0  {'Beauty': 377777}  http://ecx.images-amazon.com/images/I/41hbUxgB...   \n\n  brand                                         categories description  \\\n0  None  [Clothing, Shoes & Jewelry, Women, Shoes, Slip...        None   \n\n                                               feats  \n0  [0.0, 3.3559999466, 0.0, 0.0, 1.5535999537, 0....  "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewerName</th>\n      <th>helpful</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>reviewTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A2IQ3AQHJNJUNZ</td>\n      <td>8921463216</td>\n      <td>GirlGamer</td>\n      <td>[0, 0]</td>\n      <td>Everbody loves my sandals, a little narrow aro...</td>\n      <td>5</td>\n      <td>Love Love Love</td>\n      <td>1395014400</td>\n      <td>03 17, 2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   index      reviewerID        asin reviewerName helpful  \\\n0      0  A2IQ3AQHJNJUNZ  8921463216    GirlGamer  [0, 0]   \n\n                                          reviewText  overall         summary  \\\n0  Everbody loves my sandals, a little narrow aro...        5  Love Love Love   \n\n   unixReviewTime   reviewTime  \n0      1395014400  03 17, 2014  "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(meta), len(rev))\r\n",
    "display(meta[:1])\r\n",
    "display(rev[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, a = meta[\"index\"], meta[\"asin\"]\r\n",
    "asin_to_idx = dict(zip(a, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165947it [00:07, 21822.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rPU = defaultdict(list)\r\n",
    "# positive pair list:\r\n",
    "for idx, b in tqdm.tqdm(rev.iterrows()):\r\n",
    "    asin, user = b[\"asin\"], b[\"reviewerName\"]\r\n",
    "    rPU[user].append(asin)\r\n",
    "for ratList in rPU.values():\r\n",
    "    ratList.sort()\r\n",
    "# 121084 - name, 146593 - user\r\n",
    "print(len(rPU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121084/121084 [00:01<00:00, 105427.82it/s]\n"
     ]
    }
   ],
   "source": [
    "all_items = set(meta[\"asin\"])\r\n",
    "positives = set()\r\n",
    "for user, rats in tqdm.tqdm(rPU.items()):\r\n",
    "    for i in range(len(rats)):\r\n",
    "        for j in range(i + 1, len(rats)):\r\n",
    "            positives.add((rats[i], rats[j]))\r\n",
    "positives_li = list(positives)\r\n",
    "ones = [1] * len(positives_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num positives 2140082 Maximum possible (using combinatorics) 3687163.0\n",
      "X_tr: 1712278, X_va: 213795, X_te: 214009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# 80 - 10 - 10\r\n",
    "# 90 - 10\r\n",
    "# === THESE ARE ONLY POSITIVES SAMPLES\r\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(positives_li, ones, test_size=0.1, random_state=1)\r\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.111, random_state=1) # 0.25 x 0.8 = 0.2\r\n",
    "print(\"Num positives\", len(positives), \"Maximum possible (using combinatorics)\", sum([len(t) * (len(t) - 1) / 2 for t in rPU.values() if len(t) > 1]))\r\n",
    "print(f\"X_tr: {len(X_tr)}, X_va: {len(X_va)}, X_te: {len(X_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data import Dataset\r\n",
    "import pytorch_lightning as pl\r\n",
    "import sklearn\r\n",
    "import random\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosNegDataset(Dataset):\r\n",
    "    # pos_list: positives in dataset\r\n",
    "    # asins: all asins in dataset\r\n",
    "    # all_pos: all positives in the entire dataset\r\n",
    "    # features: all features as df\r\n",
    "    # asin_to_idx: map asin to an idx in features\r\n",
    "    def __init__(self, pos_list, all_pos, all_asins, features, asin_to_idx):\r\n",
    "        self.pos_list = pos_list\r\n",
    "        self.all_pos = all_pos\r\n",
    "        self.all_asins = all_asins\r\n",
    "        self.features = features\r\n",
    "        self.asin_to_idx = asin_to_idx\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.pos_list) * 2\r\n",
    "\r\n",
    "    def get_feature(self, asin):\r\n",
    "        idx = self.asin_to_idx[asin]\r\n",
    "        return torch.tensor(self.features.loc[idx][\"feats\"])\r\n",
    "    def generate_neg_pair(self):\r\n",
    "        pair = random.sample(self.all_asins, 2)\r\n",
    "        pair.sort()\r\n",
    "        pair = tuple(pair)\r\n",
    "        if pair in self.all_pos:\r\n",
    "            pair = random.sample(self.all_asins, 2)\r\n",
    "            pair.sort()\r\n",
    "            pair = tuple(pair)\r\n",
    "        return pair\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        if idx < len(self.pos_list):\r\n",
    "            a, b = self.pos_list[idx]\r\n",
    "            return self.get_feature(a), self.get_feature(b), 1\r\n",
    "        a, b = self.generate_neg_pair()\r\n",
    "        return self.get_feature(a), self.get_feature(b), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mahalanobis(pl.LightningModule):\r\n",
    "    def __init__(self, embedding_dims = 4096, K = 10, c = 2):\r\n",
    "        super().__init__()\r\n",
    "        self.mahal = nn.Linear(embedding_dims, K)\r\n",
    "        self.l = nn.BCEWithLogitsLoss()\r\n",
    "        self.c = c\r\n",
    "    def forward(self, user_input, item_input):\r\n",
    "        # bs x k\r\n",
    "        a = self.mahal(user_input - item_input)\r\n",
    "        b = torch.linalg.norm(a, dim=1)\r\n",
    "        out = b - self.c\r\n",
    "        return out\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        i1, i2, l = batch\r\n",
    "        pred = self(i1, i2)\r\n",
    "        loss = self.l(pred, l.float())\r\n",
    "        self.log(\"tr/loss_step\", loss)\r\n",
    "        return loss\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        i1, i2, l = batch\r\n",
    "        pred = self(i1, i2)\r\n",
    "        loss = self.l(pred, l.float()).item()\r\n",
    "        accuracy = sklearn.metrics.accuracy_score(l.cpu(), pred.cpu().flatten() > 0.0)\r\n",
    "        self.log(\"val/loss_step\", loss)\r\n",
    "        self.log(\"val/acc_step\", accuracy)\r\n",
    "        return {\"val_acc\": accuracy, \"val_loss\": loss, \"pred\": pred.cpu().flatten().numpy(), \"label\": l.cpu().flatten().numpy()}\r\n",
    "    def training_epoch_end(self, training_step_outputs):\r\n",
    "        avg_loss = np.mean(np.mean(training_step_outputs))\r\n",
    "        self.log(\"tr/loss_epoch\", avg_loss)\r\n",
    "    def validation_epoch_end(self, validation_step_outputs):\r\n",
    "        acc, loss, pred, label = zip(*[(t[\"val_acc\"], t[\"val_loss\"], t[\"pred\"], t[\"label\"]) for t in validation_step_outputs])\r\n",
    "        mean_acc, mean_loss = np.mean(acc), np.mean(loss)\r\n",
    "        pred, label = np.array(pred).flatten(), np.array(label).flatten()\r\n",
    "        self.log(\"val/loss_epoch\", mean_loss)\r\n",
    "        self.log(\"val/acc_epoch\", sklearn.metrics.accuracy_score(label, pred > 0.0))\r\n",
    "        self.log(\"val/f1_epoch\", sklearn.metrics.f1_score(label, pred > 0.0))\r\n",
    "    def configure_optimizers(self):\r\n",
    "        return torch.optim.Adam(self.parameters())\r\n",
    "\r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(PosNegDataset(X_tr, positives, all_items, meta, asin_to_idx),\r\n",
    "                          batch_size=128, num_workers=0, shuffle=True)\r\n",
    "    def val_dataloader(self):\r\n",
    "        return DataLoader(PosNegDataset(X_va, positives, all_items, meta, asin_to_idx),\r\n",
    "                          batch_size=128, num_workers=0, shuffle=True)\r\n",
    "    \r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3460, 0.0000, 0.5135,  ..., 0.0000, 0.0000, 0.0000]) tensor([0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.8614, 0.0000]) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "tr_ds = PosNegDataset(X_tr, positives, all_items, meta, asin_to_idx)\r\n",
    "tr_dl = DataLoader(tr_ds, batch_size = 25, num_workers = 0)\r\n",
    "for i, j, l in tr_dl:\r\n",
    "    print(i[0], j[0], l[0])\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:89: LightningDeprecationWarning: `reload_dataloaders_every_epoch` is deprecated in v1.4 and will be removed in v1.6. Please use `reload_dataloaders_every_n_epochs` in Trainer.\n",
      "  \"`reload_dataloaders_every_epoch` is deprecated in v1.4 and will be removed in v1.6.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | mahal | Linear            | 41.0 K\n",
      "1 | l     | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "41.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.0 K    Total params\n",
      "0.164     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:442: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  f\"Your `{mode.dataloader_prefix}_dataloader` has `shuffle=True`,\"\n",
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 100/30096 [00:14<1:13:41,  6.78it/s, loss=0.667, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\envs\\cse158_37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "model = Mahalanobis()\r\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/acc_step\")\r\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1, reload_dataloaders_every_epoch=True, progress_bar_refresh_rate=50, logger=True, default_root_dir=\"./models\", callbacks=[checkpoint_callback])\r\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cse158_37': conda)",
   "name": "python3711jvsc74a57bd00d151ab34c9aaab944e38038fcabbcbde94c7261585d9a9c92994237575effe2"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}